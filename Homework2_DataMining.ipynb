{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines & Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problema propusa este identificarea autorului unui text si gasirea similaritatilor dintre carti, analizand stilul de scriere al autorilor comparand diferite modele cu diversi hiper-parametrii. Setul de date contine mai mlte texte ale fiecarui autor.\n",
    "\n",
    "Modelele ce trebuie folosite si comparate sunt: SVM, DBCAN, KMeans, Agglomerative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citirea setului de date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(\"Ciolacu C. Florentina-Neluta.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encodarea autorilor\n",
    "\n",
    "Encodarea autorilor cu numere de la 0 la 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = list(dataset.columns.values)\n",
    "authors.pop(0)\n",
    "authors_dict = {author: index for index, author in enumerate(authors)}\n",
    "\n",
    "new_dataset = pd.DataFrame(columns=[\"text\", \"author\"])\n",
    "for el in authors_dict:\n",
    "    aux = pd.DataFrame(columns=[\"text\"])\n",
    "    aux[\"text\"] = dataset[el]\n",
    "    aux[\"author\"] = pd.Series([authors_dict.get(el)] * len(dataset[el]))\n",
    "    new_dataset = new_dataset.append(aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impartirea setului de date in features si labels\n",
    "\n",
    "Am ales algoritmul TfidfVectorizer pentru extragerea feature-urilor din texte.\n",
    "De asemenea, am renuntat la cuvintele din alte limbi in afara de engleza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(strip_accents='ascii', stop_words='english', token_pattern=r'(?u)\\b[A-Za-z]+\\b')\n",
    "X = vectorizer.fit_transform(new_dataset.text)\n",
    "y = new_dataset.author\n",
    "y = y.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vizualizarea datelor\n",
    "\n",
    "Plotarea in functie de autor. Reducerea dimensionalitatii pentru features a fost facuta cu TruncatedSVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "classifier = TruncatedSVD(n_components=3)\n",
    "Xplot = classifier.fit_transform(X)\n",
    "Xplot = pd.DataFrame(Xplot)\n",
    "yplot = pd.DataFrame(y)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.scatter(Xplot[0], Xplot[1], Xplot[2], c=yplot[\"author\"], s=50,\n",
    "           cmap=\"gnuplot\",\n",
    "           edgecolor=\"black\", linewidth=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Support Vector Machines\n",
    "\n",
    "Text Classification folosind Support Vector Machines pentru a prezice autorul fiecarui text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impartirea setului de date in train si test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determinarea scorului cel mai bun\n",
    "\n",
    "Pentru a obtine cel mai bun scor, am variat modelele prin setul de parametrii(kernel, C, gamma) si am afisat scorurile obtinute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "best_svm_model = (None, None, -1.0, -1.0, -1.0)\n",
    "\n",
    "for i, (Model, kwargs) in enumerate([(SVC, {\"C\": 1.0, \"gamma\": 0.0001, \"kernel\": \"rbf\"}), (SVC, {\"C\": 1.0, \"gamma\": 1.0, \"kernel\": \"rbf\"}), (SVC, {\"C\": 0.1, \"gamma\": 0.0001, \"kernel\": \"rbf\"}), (SVC, {\"C\": 0.5, \"gamma\": 1.0, \"kernel\": \"rbf\"}),\n",
    "                                     (SVC, {\"C\": 1.0, \"gamma\": 0.0001, \"kernel\": \"linear\"}), (SVC, {\"C\": 1.0, \"gamma\": 1.0, \"kernel\": \"linear\"}),\n",
    "                                     (SVC, {\"C\": 1.0, \"gamma\": 0.0001, \"kernel\": \"poly\"}), (SVC, {\"C\": 1.0, \"gamma\": 1.0, \"kernel\": \"poly\"}), ]):\n",
    "    accuracy = 0.0\n",
    "    precision = 0.0\n",
    "    f1_score_ = 0.0\n",
    "    model = Model(**kwargs)\n",
    "    model.fit(x_train, y_train)\n",
    "    accuracy = accuracy_score(y_test, model.predict(x_test))\n",
    "    precision = precision_score(y_test, model.predict(x_test), average=\"weighted\")\n",
    "    f1_score_ = f1_score(y_test, model.predict(x_test), average=\"weighted\")\n",
    "    if best_svm_model[2] < accuracy:\n",
    "        best_svm_model = (model, kwargs, accuracy, precision, f1_score_)\n",
    "    print(Model.__name__, kwargs)\n",
    "    print(\"Average accuracy:\", accuracy)\n",
    "    print(\"Average precision:\", precision)\n",
    "    print(\"Average f1 score:\", f1_score_)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best results were with the model: \", best_svm_model[1])\n",
    "print(\"The scores obtained:\")\n",
    "print(\"Accuracy:\", best_svm_model[2])\n",
    "print(\"Precision:\", best_svm_model[3])\n",
    "print(\"f1 score:\", best_svm_model[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducerea dimensionalitatii\n",
    "\n",
    "Am incercat o reducere a dimensionalitatii deoarece datele aveau o dimensionalitate foarte mare. Utilizand algoritmul PCA, am verificat imbunatatirea rezultatelor.\n",
    "Concluzia este ca reducerea dimensionalitatii ajuta, intrucat scorurile obtinute au fost mai mari. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_dimensions_list = [400, 200, 100]\n",
    "best_svm_model = [(None, None, -1.0, -1.0, -1.0)] * len(pca_dimensions_list)\n",
    "\n",
    "for j, pca_dimensions in enumerate(pca_dimensions_list):\n",
    "    clf = TruncatedSVD(n_components=pca_dimensions)\n",
    "    Xpca = clf.fit_transform(X)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(Xpca, y)\n",
    "    \n",
    "    print(\"For PCA with:\", pca_dimensions)\n",
    "    \n",
    "    for i, (Model, kwargs) in enumerate([(SVC, {\"C\": 1.0, \"gamma\": 0.0001, \"kernel\": \"rbf\"}), (SVC, {\"C\": 1.0, \"gamma\": 1.0, \"kernel\": \"rbf\"}), (SVC, {\"C\": 0.1, \"gamma\": 0.0001, \"kernel\": \"rbf\"}), (SVC, {\"C\": 0.5, \"gamma\": 1.0, \"kernel\": \"rbf\"}),\n",
    "                                         (SVC, {\"C\": 1.0, \"gamma\": 0.0001, \"kernel\": \"linear\"}), (SVC, {\"C\": 1.0, \"gamma\": 1.0, \"kernel\": \"linear\"}),\n",
    "                                         (SVC, {\"C\": 1.0, \"gamma\": 0.0001, \"kernel\": \"poly\"}), (SVC, {\"C\": 1.0, \"gamma\": 1.0, \"kernel\": \"poly\"}),]):\n",
    "        accuracy = 0.0\n",
    "        precision = 0.0\n",
    "        f1_score_ = 0.0\n",
    "        model = Model(**kwargs)\n",
    "        model.fit(x_train, y_train)\n",
    "        accuracy = accuracy_score(y_test, model.predict(x_test))\n",
    "        precision = precision_score(y_test, model.predict(x_test), average=\"weighted\")\n",
    "        f1_score_ = f1_score(y_test, model.predict(x_test), average=\"weighted\")\n",
    "        if best_svm_model[j][2] < accuracy:\n",
    "            best_svm_model[j] = (model, kwargs, accuracy, precision, f1_score_)\n",
    "        print(Model.__name__, kwargs)\n",
    "        print(\"Average accuracy:\", accuracy)\n",
    "        print(\"Average precision:\", precision)\n",
    "        print(\"Average f1 score:\", f1_score_)\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, dimensions in enumerate(pca_dimensions_list):\n",
    "    print(\"The best results for PCA with dimensions:\", dimensions)\n",
    "    print(best_svm_model[i][1])\n",
    "    print(\"Accuracy:\", best_svm_model[i][2])\n",
    "    print(\"Precision:\", best_svm_model[i][3])\n",
    "    print(\"f1 score:\", best_svm_model[i][4])\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - Clustering\n",
    "\n",
    "Text Clustering folosind metodele de Clustering: DBSCAN, KMeans and Hierarchical(Agglomerative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN - Determinarea scorului cel mai bun\n",
    "\n",
    "Pentru a obtine cel mai bun scor, am variat modelele prin setul de parametrii(eps, min_samples) si am afisat scorurile obtinute.\n",
    "Metricele folosite: silhouette score, homogeneity score, completeness score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import homogeneity_score, silhouette_score, completeness_score\n",
    "\n",
    "best_dbscan_model = (None, None, -1.0, -1.0, -1.0)\n",
    "\n",
    "eps = np.arange(0.5, 1, 0.1)\n",
    "min_samples = np.arange(2, 6, 1)\n",
    "\n",
    "models = []\n",
    "\n",
    "for eps_val in eps:\n",
    "    for min_sample in min_samples:\n",
    "        models.append((DBSCAN, {\"eps\": eps_val, \"min_samples\": min_sample}))\n",
    "\n",
    "for i, (Model, kwargs) in enumerate(models):\n",
    "    homogeneity = 0.0\n",
    "    silhouette = 0.0\n",
    "    completeness = 0.0\n",
    "    model = Model(**kwargs)\n",
    "    model.fit(X)\n",
    "    unique, counts = np.unique(model.labels_, return_counts=True)\n",
    "    if len(dict(zip(unique, counts))) > 1:\n",
    "        homogeneity = homogeneity_score(y, model.labels_)\n",
    "        silhouette = silhouette_score(X, model.labels_)\n",
    "        completeness = completeness_score(y, model.labels_)\n",
    "        if best_dbscan_model[3] < silhouette:\n",
    "            best_dbscan_model = (model, kwargs, homogeneity, silhouette, completeness)\n",
    "    print(Model.__name__, kwargs)\n",
    "    print(dict(zip(unique, counts)))\n",
    "    print(\"Homogeneity: \", homogeneity)\n",
    "    print(\"Silhouette: \", silhouette)\n",
    "    print(\"Completeness: \", completeness)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best results were with the DBSCAN model: \", best_dbscan_model[1])\n",
    "print(\"The results obtained: \")\n",
    "print(\"Homogeneity:\", best_dbscan_model[2])\n",
    "print(\"Silhouette:\", best_dbscan_model[3])\n",
    "print(\"Completeness:\", best_dbscan_model[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducerea dimensionalitatii\n",
    "\n",
    "Am incercat o reducere a dimensionalitatii deoarece datele aveau o dimensionalitate foarte mare. Utilizand algoritmul PCA, am verificat imbunatatirea rezultatelor. Concluzia este ca reducerea dimensionalitatii nu aduce modificari prea mari."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_dimensions_list = [400, 200, 100]\n",
    "best_dbscan_model = [(None, None, -1.0, -1.0, -1.0)] * len(pca_dimensions_list)\n",
    "\n",
    "for j, pca_dimensions in enumerate(pca_dimensions_list):\n",
    "    clf = TruncatedSVD(n_components=pca_dimensions)\n",
    "    Xpca = clf.fit_transform(X)\n",
    "    \n",
    "    eps = np.arange(0.5, 1, 0.1)\n",
    "    min_samples = np.arange(2, 6, 1)\n",
    "    \n",
    "    models = []\n",
    "    \n",
    "    for eps_val in eps:\n",
    "        for min_sample in min_samples:\n",
    "            models.append((DBSCAN, {\"eps\": eps_val, \"min_samples\": min_sample}))\n",
    "            \n",
    "    print(\"Dimensions:\", pca_dimensions)\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    for i, (Model, kwargs) in enumerate(models):\n",
    "        homogeneity = 0.0\n",
    "        silhouette = 0.0\n",
    "        completeness = 0.0\n",
    "        model = Model(**kwargs)\n",
    "        model.fit(Xpca)\n",
    "        unique, counts = np.unique(model.labels_, return_counts=True)\n",
    "        if len(dict(zip(unique, counts))) > 1:\n",
    "            homogeneity = homogeneity_score(y, model.labels_)\n",
    "            silhouette = silhouette_score(Xpca, model.labels_)\n",
    "            completeness = completeness_score(y, model.labels_)\n",
    "            if best_dbscan_model[j][3] < silhouette:\n",
    "                best_dbscan_model[j] = (model, kwargs, homogeneity, silhouette, completeness)\n",
    "        print(Model.__name__, kwargs)\n",
    "        print(dict(zip(unique, counts)))\n",
    "        print(\"Homogeneity:\", homogeneity)\n",
    "        print(\"Silhouette:\", silhouette)\n",
    "        print(\"Completeness:\", completeness)\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, dimensions in enumerate(pca_dimensions_list):\n",
    "    print(\"The best results for PCA with dimensions:\", dimensions)\n",
    "    print(best_dbscan_model[i][1])\n",
    "    print(\"Homogeneity:\", best_dbscan_model[i][2])\n",
    "    print(\"Silhouette:\", best_dbscan_model[i][3])\n",
    "    print(\"Completeness:\", best_dbscan_model[i][4])\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans - Determinarea scorului cel mai bun\n",
    "\n",
    "Pentru a obtine cel mai bun scor, am variat modelele prin setul de parametrii(n_clusters, init) si am afisat scorurile obtinute.\n",
    "Metricele folosite: silhouette score, homogeneity score, completeness score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import homogeneity_score, silhouette_score, completeness_score\n",
    "\n",
    "best_kmeans_model = (None, None, -1.0, -1.0, -1.0)\n",
    "\n",
    "n_clusters = np.arange(10, 20, 1)\n",
    "inits = ['k-means++', 'random']\n",
    "\n",
    "models = []\n",
    "\n",
    "for n in n_clusters:\n",
    "    for init in inits:\n",
    "        models.append((KMeans, {\"n_clusters\": n, \"init\": init}))\n",
    "\n",
    "for i, (Model, kwargs) in enumerate(models):\n",
    "    print(Model.__name__, kwargs)\n",
    "    homogeneity = 0.0\n",
    "    silhouette = 0.0\n",
    "    completeness = 0.0\n",
    "    model = Model(**kwargs)\n",
    "    model.fit(X)\n",
    "    unique, counts = np.unique(model.labels_, return_counts=True)\n",
    "    homogeneity = homogeneity_score(y, model.labels_)\n",
    "    silhouette = silhouette_score(X, model.labels_)\n",
    "    completeness = completeness_score(y, model.labels_)\n",
    "    if best_kmeans_model[3] < silhouette:\n",
    "        best_kmeans_model = (model, kwargs, homogeneity, silhouette, completeness)\n",
    "    print(\"Homogeneity:\", homogeneity)\n",
    "    print(\"Silhouette:\", silhouette)\n",
    "    print(\"Completeness:\", completeness)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best results were with the KMeans model: \", best_kmeans_model[1])\n",
    "print(\"The results obtained: \")\n",
    "print(\"Homogeneity:\", best_kmeans_model[2])\n",
    "print(\"Silhouette:\", best_kmeans_model[3])\n",
    "print(\"Completeness:\", best_kmeans_model[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducerea dimensionalitatii\n",
    "\n",
    "Am incercat o reducere a dimensionalitatii deoarece datele aveau o dimensionalitate foarte mare. Utilizand algoritmul PCA, am verificat imbunatatirea rezultatelor. Concluzia este ca reducerea dimensionalitatii aduce modificari, dar in sens negativ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_dimensions_list = [400, 10, 5]\n",
    "best_kmeans_model = [(None, None, -1.0, -1.0, -1.0)] * len(pca_dimensions_list)\n",
    "\n",
    "for j, pca_dimensions in enumerate(pca_dimensions_list):\n",
    "    clf = TruncatedSVD(n_components=pca_dimensions)\n",
    "    Xpca = clf.fit_transform(X)\n",
    "    \n",
    "    n_clusters = np.arange(15, 21, 1)\n",
    "    inits = ['k-means++', 'random']\n",
    "    \n",
    "    models = []\n",
    "    \n",
    "    for n in n_clusters:\n",
    "        for init in inits:\n",
    "            models.append((KMeans, {\"n_clusters\": n, \"init\": init}))\n",
    "\n",
    "    print(\"Dimensions:\", pca_dimensions)\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    for i, (Model, kwargs) in enumerate(models):\n",
    "        print(Model.__name__, kwargs)\n",
    "        homogeneity = 0.0\n",
    "        silhouette = 0.0\n",
    "        completeness = 0.0\n",
    "        model = Model(**kwargs)\n",
    "        model.fit(Xpca)\n",
    "        unique, counts = np.unique(model.labels_, return_counts=True)\n",
    "        homogeneity = homogeneity_score(y, model.labels_)\n",
    "        silhouette = silhouette_score(Xpca, model.labels_)\n",
    "        completeness = completeness_score(y, model.labels_)\n",
    "        # get best model to print after\n",
    "        if best_kmeans_model[j][3] < silhouette:\n",
    "            best_kmeans_model[j] = (model, kwargs, homogeneity, silhouette, completeness)\n",
    "        print(\"Homogeneity:\", homogeneity)\n",
    "        print(\"Silhouette:\", silhouette)\n",
    "        print(\"Completeness:\", completeness)\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, dimensions in enumerate(pca_dimensions_list):\n",
    "    print(\"The best results for PCA with dimensions:\", dimensions)\n",
    "    print(best_kmeans_model[i][1])\n",
    "    print(\"Homogeneity:\", best_kmeans_model[i][2])\n",
    "    print(\"Silhouette:\", best_kmeans_model[i][3])\n",
    "    print(\"Completeness:\", best_kmeans_model[i][4])\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotarea modelelor cele mai bune de KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_classifier = TruncatedSVD(n_components=3)\n",
    "Xpca_plot = pca_classifier.fit_transform(X)\n",
    "Xpca_plot = pd.DataFrame(Xpca_plot)\n",
    "\n",
    "model = KMeans(n_clusters=14, init='k-means++')\n",
    "model.fit(X)\n",
    "y_plot = pd.DataFrame(model.labels_)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.scatter(Xpca_plot[0], Xpca_plot[1], Xpca_plot[2], c=y_plot[0], s=50,\n",
    "           cmap=\"gnuplot\",\n",
    "           edgecolor=\"black\", linewidth=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (Model, kwargs, dimensions) in enumerate([(KMeans, {'n_clusters': 19, 'init': 'k-means++'}, 400), (KMeans, {'n_clusters': 19, 'init': 'k-means++'}, 10), (KMeans, {'n_clusters': 17, 'init': 'k-means++'}, 5)]):\n",
    "    print(Model.__name__, kwargs, \"with: \", dimensions)\n",
    "    clf = TruncatedSVD(n_components=dimensions)\n",
    "    Xpca = clf.fit_transform(X)\n",
    "    model = Model(**kwargs)\n",
    "    model.fit(Xpca)\n",
    "    y_plot = pd.DataFrame(model.labels_)\n",
    "    fig = plt.figure(i)\n",
    "    ax = Axes3D(fig)\n",
    "    ax.scatter(Xpca_plot[0], Xpca_plot[1], Xpca_plot[2], c=y_plot[0], s=50,\n",
    "           cmap=\"gnuplot\",\n",
    "           edgecolor=\"black\", linewidth=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative - Determinarea scorului cel mai bun\n",
    "\n",
    "Pentru a obtine cel mai bun scor, am variat modelele prin setul de parametrii(n_clusters) si am afisat scorurile obtinute.\n",
    "Metricele folosite: silhouette score, homogeneity score, completeness score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import homogeneity_score, silhouette_score, completeness_score\n",
    "\n",
    "best_agglomerative_model = (None, None, -1.0, -1.0, -1.0)\n",
    "\n",
    "n_clusters = np.arange(2, 11, 1)\n",
    "\n",
    "models = []\n",
    "\n",
    "for n in n_clusters:\n",
    "    models.append((AgglomerativeClustering, {\"n_clusters\": n}))\n",
    "\n",
    "for i, (Model, kwargs) in enumerate(models):\n",
    "    print(Model.__name__, kwargs)\n",
    "    homogeneity = 0.0\n",
    "    silhouette = 0.0\n",
    "    completeness = 0.0\n",
    "    model = Model(**kwargs)\n",
    "    model.fit(X.toarray())\n",
    "    unique, counts = np.unique(model.labels_, return_counts=True)\n",
    "    homogeneity = homogeneity_score(y, model.labels_)\n",
    "    silhouette = silhouette_score(X, model.labels_)\n",
    "    completeness = completeness_score(y, model.labels_)\n",
    "    if best_agglomerative_model[3] < silhouette:\n",
    "        best_agglomerative_model = (model, kwargs, homogeneity, silhouette, completeness)\n",
    "    print(\"Homogeneity:\", homogeneity)\n",
    "    print(\"Silhouette:\", silhouette)\n",
    "    print(\"Completeness:\", completeness)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best results were with the Agglomerative model: \", best_agglomerative_model[1])\n",
    "print(\"The results obtained: \")\n",
    "print(\"Homogeneity:\", best_agglomerative_model[2])\n",
    "print(\"Silhouette:\", best_agglomerative_model[3])\n",
    "print(\"Completeness:\", best_agglomerative_model[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducerea dimensionalitatii\n",
    "\n",
    "Am incercat o reducere a dimensionalitatii deoarece datele aveau o dimensionalitate foarte mare. Utilizand algoritmul PCA, am verificat imbunatatirea rezultatelor. Concluzia este ca reducerea dimensionalitatii nu aduce modificari prea mari."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_dimensions_list = [400, 10, 5]\n",
    "best_agglomerative_model = [(None, None, -1.0, -1.0, -1.0)] * len(pca_dimensions_list)\n",
    "\n",
    "for j, pca_dimensions in enumerate(pca_dimensions_list):\n",
    "    clf = TruncatedSVD(n_components=pca_dimensions)\n",
    "    Xpca = clf.fit_transform(X)\n",
    "    \n",
    "    n_clusters = np.arange(2, 11, 1)\n",
    "    \n",
    "    models = []\n",
    "    \n",
    "    for n in n_clusters:\n",
    "        models.append((AgglomerativeClustering, {\"n_clusters\": n}))\n",
    "\n",
    "    print(\"Dimensions:\", pca_dimensions)\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    for i, (Model, kwargs) in enumerate(models):\n",
    "        print(Model.__name__, kwargs)\n",
    "        homogeneity = 0.0\n",
    "        silhouette = 0.0\n",
    "        completeness = 0.0\n",
    "        model = Model(**kwargs)\n",
    "        model.fit(Xpca)\n",
    "        unique, counts = np.unique(model.labels_, return_counts=True)\n",
    "        homogeneity = homogeneity_score(y, model.labels_)\n",
    "        silhouette = silhouette_score(Xpca, model.labels_)\n",
    "        completeness = completeness_score(y, model.labels_)\n",
    "        if best_agglomerative_model[j][3] < silhouette:\n",
    "            best_agglomerative_model[j] = (model, kwargs, homogeneity, silhouette, completeness)\n",
    "        print(\"Homogeneity:\", homogeneity)\n",
    "        print(\"Silhouette:\", silhouette)\n",
    "        print(\"Completeness:\", completeness)\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, dimensions in enumerate(pca_dimensions_list):\n",
    "    print(\"The best results for PCA with dimensions:\", dimensions)\n",
    "    print(best_agglomerative_model[i][1])\n",
    "    print(\"Homogeneity:\", best_agglomerative_model[i][2])\n",
    "    print(\"Silhouette:\", best_agglomerative_model[i][3])\n",
    "    print(\"Completeness:\", best_agglomerative_model[i][4])\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotarea modelelor cele mai bune de Agglomerative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AgglomerativeClustering(n_clusters=10)\n",
    "model.fit(X.toarray())\n",
    "y_plot = pd.DataFrame(model.labels_)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.scatter(Xpca_plot[0], Xpca_plot[1], Xpca_plot[2], c=y_plot[0], s=50,\n",
    "           cmap=\"gnuplot\",\n",
    "           edgecolor=\"black\", linewidth=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (Model, kwargs, dimensions) in enumerate([(AgglomerativeClustering, {'n_clusters': 10}, 400), (AgglomerativeClustering, {'n_clusters': 5}, 10), (AgglomerativeClustering, {'n_clusters': 5}, 5)]):\n",
    "    print(Model.__name__, kwargs, \"with: \", dimensions)\n",
    "    clf = TruncatedSVD(n_components=dimensions)\n",
    "    Xpca = clf.fit_transform(X)\n",
    "    model = Model(**kwargs)\n",
    "    model.fit(Xpca)\n",
    "    y_plot = pd.DataFrame(model.labels_)\n",
    "    fig = plt.figure(i)\n",
    "    ax = Axes3D(fig)\n",
    "    ax.scatter(Xpca_plot[0], Xpca_plot[1], Xpca_plot[2], c=y_plot[0], s=50,\n",
    "           cmap=\"gnuplot\",\n",
    "           edgecolor=\"white\", linewidth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
